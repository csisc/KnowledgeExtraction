{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Installation des bibliothèques Python et téléchargement du modèle de langage\n",
        "* Afin d'utiliser plus rapidement un modèle de langage, il faut recourir à l'utilisation de la carte graphique (GPU). On installe une édition ajustée de Llama-cpp-python, la bibliothèque Python qui permet l'utilisation des modèles de langage, afin de permettre ce recours.\n",
        "* Pour permettre la réutilisation des petits modèles de langage dans une machine locale, un nouveau format de fichier (GGUF) a été créé pour sauvegarder les modèles générés. Ces fichiers GGUF peuvent être retrouvés sur [Hugging Face](https://huggingface.co/).\n",
        "  * Llama-3.2 est le nom du modèle.\n",
        "  * 3B est le nombre d'hyperparamètres (Trois billions)\n",
        "  * Q8_0 est la qualité de l'entrainement (Qualité Supérieure).\n",
        "* Vous pouvez entrainer un modèle de langage sur vos données afin qu'il génère des résultats plus adaptés. Ceci s'appelle le fine-tuning. C'est une tâche à laisser aux informaticiens."
      ],
      "metadata": {
        "id": "IxJcD2S8k-Vx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --no-cache-dir llama-cpp-python==0.2.90 --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu123\n",
        "!wget https://huggingface.co/lmstudio-community/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q8_0.gguf"
      ],
      "metadata": {
        "id": "wEW8m952HcKz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b0cbe82-fd34-40e1-814e-ed9c7d61e844"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://abetlen.github.io/llama-cpp-python/whl/cu123\n",
            "Requirement already satisfied: llama-cpp-python==0.2.90 in /usr/local/lib/python3.11/dist-packages (0.2.90)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python==0.2.90) (4.12.2)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python==0.2.90) (1.26.4)\n",
            "Requirement already satisfied: diskcache>=5.6.1 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python==0.2.90) (5.6.3)\n",
            "Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python==0.2.90) (3.1.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2>=2.11.3->llama-cpp-python==0.2.90) (3.0.2)\n",
            "--2025-03-15 16:43:31--  https://huggingface.co/lmstudio-community/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q8_0.gguf\n",
            "Resolving huggingface.co (huggingface.co)... 18.239.50.16, 18.239.50.49, 18.239.50.80, ...\n",
            "Connecting to huggingface.co (huggingface.co)|18.239.50.16|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs-us-1.hf.co/repos/ee/ff/eefff9a8e8402b0d5b4511add08357762ad9df26ec0a05f1e169a74d188fb00f/94f22b7231df5cd1907ff48dba54497b2d7912a4ce60d914f3dcfc0347fa8f21?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27Llama-3.2-3B-Instruct-Q8_0.gguf%3B+filename%3D%22Llama-3.2-3B-Instruct-Q8_0.gguf%22%3B&Expires=1742060611&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0MjA2MDYxMX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zL2VlL2ZmL2VlZmZmOWE4ZTg0MDJiMGQ1YjQ1MTFhZGQwODM1Nzc2MmFkOWRmMjZlYzBhMDVmMWUxNjlhNzRkMTg4ZmIwMGYvOTRmMjJiNzIzMWRmNWNkMTkwN2ZmNDhkYmE1NDQ5N2IyZDc5MTJhNGNlNjBkOTE0ZjNkY2ZjMDM0N2ZhOGYyMT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=KJMLsm3Y7KFigCax0nikSI1yCqB7e106hBYUglx2GedK%7EX%7EbodO2F4ZWhfEgGneOjCfpOAns4KzaB0XSzFLYycJ9SIV5l0IFuXZ17cZ3U47RJCaxGH8TBlpirEwlGCvQv7npJksi2OCjqvimw%7E0CFF10zLSNsrsR69U9ZEiJS13%7Ewthc-k0pg%7EVS3edNRb-kVeJQZa64bK08ZFWRkaSAHZmHZWR1CqBTNwNlzAQhVrI3py3qjbp4CHEdnaeM6o2dnVt%7ESgs%7E4zAAwZtArXPpqRnSvRK8DZsLohCbDg8mwbw%7EKpTG8ck9WyxQXZxuz7SWxoFEHktHMOIDttFE2HIhZg__&Key-Pair-Id=K24J24Z295AEI9 [following]\n",
            "--2025-03-15 16:43:31--  https://cdn-lfs-us-1.hf.co/repos/ee/ff/eefff9a8e8402b0d5b4511add08357762ad9df26ec0a05f1e169a74d188fb00f/94f22b7231df5cd1907ff48dba54497b2d7912a4ce60d914f3dcfc0347fa8f21?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27Llama-3.2-3B-Instruct-Q8_0.gguf%3B+filename%3D%22Llama-3.2-3B-Instruct-Q8_0.gguf%22%3B&Expires=1742060611&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0MjA2MDYxMX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zL2VlL2ZmL2VlZmZmOWE4ZTg0MDJiMGQ1YjQ1MTFhZGQwODM1Nzc2MmFkOWRmMjZlYzBhMDVmMWUxNjlhNzRkMTg4ZmIwMGYvOTRmMjJiNzIzMWRmNWNkMTkwN2ZmNDhkYmE1NDQ5N2IyZDc5MTJhNGNlNjBkOTE0ZjNkY2ZjMDM0N2ZhOGYyMT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=KJMLsm3Y7KFigCax0nikSI1yCqB7e106hBYUglx2GedK%7EX%7EbodO2F4ZWhfEgGneOjCfpOAns4KzaB0XSzFLYycJ9SIV5l0IFuXZ17cZ3U47RJCaxGH8TBlpirEwlGCvQv7npJksi2OCjqvimw%7E0CFF10zLSNsrsR69U9ZEiJS13%7Ewthc-k0pg%7EVS3edNRb-kVeJQZa64bK08ZFWRkaSAHZmHZWR1CqBTNwNlzAQhVrI3py3qjbp4CHEdnaeM6o2dnVt%7ESgs%7E4zAAwZtArXPpqRnSvRK8DZsLohCbDg8mwbw%7EKpTG8ck9WyxQXZxuz7SWxoFEHktHMOIDttFE2HIhZg__&Key-Pair-Id=K24J24Z295AEI9\n",
            "Resolving cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)... 18.239.50.128, 18.239.50.73, 18.239.50.9, ...\n",
            "Connecting to cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)|18.239.50.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3421899040 (3.2G) [binary/octet-stream]\n",
            "Saving to: ‘Llama-3.2-3B-Instruct-Q8_0.gguf.2’\n",
            "\n",
            "Llama-3.2-3B-Instru 100%[===================>]   3.19G   212MB/s    in 20s     \n",
            "\n",
            "2025-03-15 16:43:51 (165 MB/s) - ‘Llama-3.2-3B-Instruct-Q8_0.gguf.2’ saved [3421899040/3421899040]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import des bibliothèques Python\n",
        "* On utilise `llama-cpp-python` pour l'utilisation des petits modèles de langage.\n",
        "* On utilise `pathlib` pour la gestion des dossiers dans notre environnement.\n"
      ],
      "metadata": {
        "id": "sgzacddRoGS-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_cpp import Llama\n",
        "import pathlib"
      ],
      "metadata": {
        "id": "6qv_jiMHgEyF"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Définition des variables et du modèle\n",
        "* On définit les données bibliographiques qu'on va utiliser dans la requête (prompt). Dans cet exemple, on utilise l'Abstract pour retrouver les termes scientifiques qu'il contient.\n",
        "* On définit le modèle qu'on va utiliser en utilisant la fonction `Llama`:\n",
        "  * `model_path`: L'endroit où se retrouve le fichier GGUF de notre modèle.\n",
        "  * `n_ctx`: Le nombre maximal des tokens (mots) à considérer dans la requête.\n",
        "  * `n_gpu_layers`: Les ressources de la carte graphique qu'on va mobiliser.\n",
        "* Dans un contexte de production, on utilise un tableau (DataFrame) pour mettre les données bibliographiques pour chaque article et on les traite en utilisant Pandas."
      ],
      "metadata": {
        "id": "wXsQCQ1zodNn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "abstract = \"\"\"The development of neural techniques has opened up new avenues for research in machine translation. Today, neural machine translation (NMT) systems can leverage highly multilingual capacities and even perform zero-shot translation, delivering promising results in terms of language coverage and quality. However, scaling quality NMT requires large volumes of parallel bilingual data, which are not equally available for the 7,000+ languages in the world.\n",
        "Focusing on improving the translation qualities of a relatively small group of high-resource languages comes at the expense of directing research attention to low-resource languages, exacerbating digital inequities in the long run.\n",
        "To break this pattern, here we introduce No Language Left Behind—a single massively multilingual model that leverages transfer learning across languages.\n",
        "We developed a conditional computational model based on the Sparsely Gated Mixture of Experts architecture2–7, which we trained on data obtained with new mining techniques tailored for low-resource languages.\n",
        "Furthermore, we devised multiple architectural and training improvements to counteract overfitting while training on thousands of tasks.\n",
        "We evaluated the performance of our model over 40,000 translation directions using tools created specifically for this purpose—an automatic benchmark (FLORES-200), a human evaluation metric (XSTS) and a toxicity detector that covers every language in our model.\n",
        "Compared with the previous state-of-the-art models, our model achieves an average of 44% improvement in translation quality as measured by BLEU.\n",
        "By demonstrating how to scale NMT to 200 languages and making all contributions in this effort freely available for non-commercial use, our work lays important groundwork for the development of a universal translation system.\"\"\""
      ],
      "metadata": {
        "id": "ZL-6JtRBg29f"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = \"Llama-3.2-3B-Instruct-Q8_0.gguf\""
      ],
      "metadata": {
        "id": "08tp2_z2hP_k"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Defining models\n",
        "MODEL_Q8_0 = Llama(\n",
        "    model_path=model, n_ctx=2048,\n",
        "    n_gpu_layers=128, logits_all=True\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55TUo1VLhVeY",
        "outputId": "0e327b1b-2b43-466e-90f7-5aecf0470a6e"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_model_loader: loaded meta data with 31 key-value pairs and 255 tensors from Llama-3.2-3B-Instruct-Q8_0.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 3B Instruct\n",
            "llama_model_loader: - kv   3:                           general.finetune str              = Instruct\n",
            "llama_model_loader: - kv   4:                           general.basename str              = Llama-3.2\n",
            "llama_model_loader: - kv   5:                         general.size_label str              = 3B\n",
            "llama_model_loader: - kv   6:                            general.license str              = llama3.2\n",
            "llama_model_loader: - kv   7:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\n",
            "llama_model_loader: - kv   8:                          general.languages arr[str,8]       = [\"en\", \"de\", \"fr\", \"it\", \"pt\", \"hi\", ...\n",
            "llama_model_loader: - kv   9:                          llama.block_count u32              = 28\n",
            "llama_model_loader: - kv  10:                       llama.context_length u32              = 131072\n",
            "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 3072\n",
            "llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 8192\n",
            "llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 24\n",
            "llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000\n",
            "llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  17:                 llama.attention.key_length u32              = 128\n",
            "llama_model_loader: - kv  18:               llama.attention.value_length u32              = 128\n",
            "llama_model_loader: - kv  19:                          general.file_type u32              = 7\n",
            "llama_model_loader: - kv  20:                           llama.vocab_size u32              = 128256\n",
            "llama_model_loader: - kv  21:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv  22:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  23:                         tokenizer.ggml.pre str              = llama-bpe\n",
            "llama_model_loader: - kv  24:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  25:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  26:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
            "llama_model_loader: - kv  27:                tokenizer.ggml.bos_token_id u32              = 128000\n",
            "llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 128009\n",
            "llama_model_loader: - kv  29:                    tokenizer.chat_template str              = {{- bos_token }}\\n{%- if custom_tools ...\n",
            "llama_model_loader: - kv  30:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   58 tensors\n",
            "llama_model_loader: - type q8_0:  197 tensors\n",
            "llm_load_vocab: special tokens cache size = 256\n",
            "llm_load_vocab: token to piece cache size = 0.7999 MB\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = BPE\n",
            "llm_load_print_meta: n_vocab          = 128256\n",
            "llm_load_print_meta: n_merges         = 280147\n",
            "llm_load_print_meta: vocab_only       = 0\n",
            "llm_load_print_meta: n_ctx_train      = 131072\n",
            "llm_load_print_meta: n_embd           = 3072\n",
            "llm_load_print_meta: n_layer          = 28\n",
            "llm_load_print_meta: n_head           = 24\n",
            "llm_load_print_meta: n_head_kv        = 8\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_swa            = 0\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 3\n",
            "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
            "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 8192\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 500000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_ctx_orig_yarn  = 131072\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
            "llm_load_print_meta: model type       = ?B\n",
            "llm_load_print_meta: model ftype      = Q8_0\n",
            "llm_load_print_meta: model params     = 3.21 B\n",
            "llm_load_print_meta: model size       = 3.18 GiB (8.50 BPW) \n",
            "llm_load_print_meta: general.name     = Llama 3.2 3B Instruct\n",
            "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
            "llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'\n",
            "llm_load_print_meta: LF token         = 128 'Ä'\n",
            "llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n",
            "llm_load_print_meta: max token length = 256\n",
            "llm_load_tensors: ggml ctx size =    0.24 MiB\n",
            "llm_load_tensors: offloading 28 repeating layers to GPU\n",
            "llm_load_tensors: offloading non-repeating layers to GPU\n",
            "llm_load_tensors: offloaded 29/29 layers to GPU\n",
            "llm_load_tensors:        CPU buffer size =   399.23 MiB\n",
            "llm_load_tensors:      CUDA0 buffer size =  3255.91 MiB\n",
            ".................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 2048\n",
            "llama_new_context_with_model: n_batch    = 512\n",
            "llama_new_context_with_model: n_ubatch   = 512\n",
            "llama_new_context_with_model: flash_attn = 0\n",
            "llama_new_context_with_model: freq_base  = 500000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:      CUDA0 KV buffer size =   224.00 MiB\n",
            "llama_new_context_with_model: KV self size  =  224.00 MiB, K (f16):  112.00 MiB, V (f16):  112.00 MiB\n",
            "llama_new_context_with_model:  CUDA_Host  output buffer size =     0.49 MiB\n",
            "llama_new_context_with_model:      CUDA0 compute buffer size =   256.50 MiB\n",
            "llama_new_context_with_model:  CUDA_Host compute buffer size =    10.01 MiB\n",
            "llama_new_context_with_model: graph nodes  = 902\n",
            "llama_new_context_with_model: graph splits = 2\n",
            "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
            "Model metadata: {'tokenizer.chat_template': '{{- bos_token }}\\n{%- if custom_tools is defined %}\\n    {%- set tools = custom_tools %}\\n{%- endif %}\\n{%- if not tools_in_user_message is defined %}\\n    {%- set tools_in_user_message = true %}\\n{%- endif %}\\n{%- if not date_string is defined %}\\n    {%- if strftime_now is defined %}\\n        {%- set date_string = strftime_now(\"%d %b %Y\") %}\\n    {%- else %}\\n        {%- set date_string = \"26 Jul 2024\" %}\\n    {%- endif %}\\n{%- endif %}\\n{%- if not tools is defined %}\\n    {%- set tools = none %}\\n{%- endif %}\\n\\n{#- This block extracts the system message, so we can slot it into the right place. #}\\n{%- if messages[0][\\'role\\'] == \\'system\\' %}\\n    {%- set system_message = messages[0][\\'content\\']|trim %}\\n    {%- set messages = messages[1:] %}\\n{%- else %}\\n    {%- set system_message = \"\" %}\\n{%- endif %}\\n\\n{#- System message #}\\n{{- \"<|start_header_id|>system<|end_header_id|>\\\\n\\\\n\" }}\\n{%- if tools is not none %}\\n    {{- \"Environment: ipython\\\\n\" }}\\n{%- endif %}\\n{{- \"Cutting Knowledge Date: December 2023\\\\n\" }}\\n{{- \"Today Date: \" + date_string + \"\\\\n\\\\n\" }}\\n{%- if tools is not none and not tools_in_user_message %}\\n    {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\\n    {{- \\'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.\\' }}\\n    {{- \"Do not use variables.\\\\n\\\\n\" }}\\n    {%- for t in tools %}\\n        {{- t | tojson(indent=4) }}\\n        {{- \"\\\\n\\\\n\" }}\\n    {%- endfor %}\\n{%- endif %}\\n{{- system_message }}\\n{{- \"<|eot_id|>\" }}\\n\\n{#- Custom tools are passed in a user message with some extra guidance #}\\n{%- if tools_in_user_message and not tools is none %}\\n    {#- Extract the first user message so we can plug it in here #}\\n    {%- if messages | length != 0 %}\\n        {%- set first_user_message = messages[0][\\'content\\']|trim %}\\n        {%- set messages = messages[1:] %}\\n    {%- else %}\\n        {{- raise_exception(\"Cannot put tools in the first user message when there\\'s no first user message!\") }}\\n{%- endif %}\\n    {{- \\'<|start_header_id|>user<|end_header_id|>\\\\n\\\\n\\' -}}\\n    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\\n    {{- \"with its proper arguments that best answers the given prompt.\\\\n\\\\n\" }}\\n    {{- \\'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.\\' }}\\n    {{- \"Do not use variables.\\\\n\\\\n\" }}\\n    {%- for t in tools %}\\n        {{- t | tojson(indent=4) }}\\n        {{- \"\\\\n\\\\n\" }}\\n    {%- endfor %}\\n    {{- first_user_message + \"<|eot_id|>\"}}\\n{%- endif %}\\n\\n{%- for message in messages %}\\n    {%- if not (message.role == \\'ipython\\' or message.role == \\'tool\\' or \\'tool_calls\\' in message) %}\\n        {{- \\'<|start_header_id|>\\' + message[\\'role\\'] + \\'<|end_header_id|>\\\\n\\\\n\\'+ message[\\'content\\'] | trim + \\'<|eot_id|>\\' }}\\n    {%- elif \\'tool_calls\\' in message %}\\n        {%- if not message.tool_calls|length == 1 %}\\n            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\\n        {%- endif %}\\n        {%- set tool_call = message.tool_calls[0].function %}\\n        {{- \\'<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n\\' -}}\\n        {{- \\'{\"name\": \"\\' + tool_call.name + \\'\", \\' }}\\n        {{- \\'\"parameters\": \\' }}\\n        {{- tool_call.arguments | tojson }}\\n        {{- \"}\" }}\\n        {{- \"<|eot_id|>\" }}\\n    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\\n        {{- \"<|start_header_id|>ipython<|end_header_id|>\\\\n\\\\n\" }}\\n        {%- if message.content is mapping or message.content is iterable %}\\n            {{- message.content | tojson }}\\n        {%- else %}\\n            {{- message.content }}\\n        {%- endif %}\\n        {{- \"<|eot_id|>\" }}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- \\'<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n\\' }}\\n{%- endif %}\\n', 'tokenizer.ggml.eos_token_id': '128009', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'gpt2', 'llama.rope.dimension_count': '128', 'llama.vocab_size': '128256', 'general.file_type': '7', 'llama.attention.key_length': '128', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.freq_base': '500000.000000', 'general.architecture': 'llama', 'general.basename': 'Llama-3.2', 'tokenizer.ggml.bos_token_id': '128000', 'llama.attention.head_count': '24', 'tokenizer.ggml.pre': 'llama-bpe', 'llama.context_length': '131072', 'general.name': 'Llama 3.2 3B Instruct', 'general.finetune': 'Instruct', 'general.type': 'model', 'general.size_label': '3B', 'llama.attention.value_length': '128', 'general.license': 'llama3.2', 'llama.feed_forward_length': '8192', 'llama.embedding_length': '3072', 'llama.block_count': '28', 'llama.attention.head_count_kv': '8'}\n",
            "Available chat formats from metadata: chat_template.default\n",
            "Using gguf chat template: {{- bos_token }}\n",
            "{%- if custom_tools is defined %}\n",
            "    {%- set tools = custom_tools %}\n",
            "{%- endif %}\n",
            "{%- if not tools_in_user_message is defined %}\n",
            "    {%- set tools_in_user_message = true %}\n",
            "{%- endif %}\n",
            "{%- if not date_string is defined %}\n",
            "    {%- if strftime_now is defined %}\n",
            "        {%- set date_string = strftime_now(\"%d %b %Y\") %}\n",
            "    {%- else %}\n",
            "        {%- set date_string = \"26 Jul 2024\" %}\n",
            "    {%- endif %}\n",
            "{%- endif %}\n",
            "{%- if not tools is defined %}\n",
            "    {%- set tools = none %}\n",
            "{%- endif %}\n",
            "\n",
            "{#- This block extracts the system message, so we can slot it into the right place. #}\n",
            "{%- if messages[0]['role'] == 'system' %}\n",
            "    {%- set system_message = messages[0]['content']|trim %}\n",
            "    {%- set messages = messages[1:] %}\n",
            "{%- else %}\n",
            "    {%- set system_message = \"\" %}\n",
            "{%- endif %}\n",
            "\n",
            "{#- System message #}\n",
            "{{- \"<|start_header_id|>system<|end_header_id|>\\n\\n\" }}\n",
            "{%- if tools is not none %}\n",
            "    {{- \"Environment: ipython\\n\" }}\n",
            "{%- endif %}\n",
            "{{- \"Cutting Knowledge Date: December 2023\\n\" }}\n",
            "{{- \"Today Date: \" + date_string + \"\\n\\n\" }}\n",
            "{%- if tools is not none and not tools_in_user_message %}\n",
            "    {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\n",
            "    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n",
            "    {{- \"Do not use variables.\\n\\n\" }}\n",
            "    {%- for t in tools %}\n",
            "        {{- t | tojson(indent=4) }}\n",
            "        {{- \"\\n\\n\" }}\n",
            "    {%- endfor %}\n",
            "{%- endif %}\n",
            "{{- system_message }}\n",
            "{{- \"<|eot_id|>\" }}\n",
            "\n",
            "{#- Custom tools are passed in a user message with some extra guidance #}\n",
            "{%- if tools_in_user_message and not tools is none %}\n",
            "    {#- Extract the first user message so we can plug it in here #}\n",
            "    {%- if messages | length != 0 %}\n",
            "        {%- set first_user_message = messages[0]['content']|trim %}\n",
            "        {%- set messages = messages[1:] %}\n",
            "    {%- else %}\n",
            "        {{- raise_exception(\"Cannot put tools in the first user message when there's no first user message!\") }}\n",
            "{%- endif %}\n",
            "    {{- '<|start_header_id|>user<|end_header_id|>\\n\\n' -}}\n",
            "    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\n",
            "    {{- \"with its proper arguments that best answers the given prompt.\\n\\n\" }}\n",
            "    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n",
            "    {{- \"Do not use variables.\\n\\n\" }}\n",
            "    {%- for t in tools %}\n",
            "        {{- t | tojson(indent=4) }}\n",
            "        {{- \"\\n\\n\" }}\n",
            "    {%- endfor %}\n",
            "    {{- first_user_message + \"<|eot_id|>\"}}\n",
            "{%- endif %}\n",
            "\n",
            "{%- for message in messages %}\n",
            "    {%- if not (message.role == 'ipython' or message.role == 'tool' or 'tool_calls' in message) %}\n",
            "        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' }}\n",
            "    {%- elif 'tool_calls' in message %}\n",
            "        {%- if not message.tool_calls|length == 1 %}\n",
            "            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\n",
            "        {%- endif %}\n",
            "        {%- set tool_call = message.tool_calls[0].function %}\n",
            "        {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n",
            "        {{- '{\"name\": \"' + tool_call.name + '\", ' }}\n",
            "        {{- '\"parameters\": ' }}\n",
            "        {{- tool_call.arguments | tojson }}\n",
            "        {{- \"}\" }}\n",
            "        {{- \"<|eot_id|>\" }}\n",
            "    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\n",
            "        {{- \"<|start_header_id|>ipython<|end_header_id|>\\n\\n\" }}\n",
            "        {%- if message.content is mapping or message.content is iterable %}\n",
            "            {{- message.content | tojson }}\n",
            "        {%- else %}\n",
            "            {{- message.content }}\n",
            "        {%- endif %}\n",
            "        {{- \"<|eot_id|>\" }}\n",
            "    {%- endif %}\n",
            "{%- endfor %}\n",
            "{%- if add_generation_prompt %}\n",
            "    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n",
            "{%- endif %}\n",
            "\n",
            "Using chat eos_token: <|eot_id|>\n",
            "Using chat bos_token: <|begin_of_text|>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Définition du prompt et exécution de la requête\n",
        "Lorsque vous écrivez un prompt, veuillez garder en tête ces points:\n",
        "* Utiliser le format Contexte - Instruction - Structure Voulue du Résultat pour expliquer ce que le petit modèle du langage doit faire.\n",
        "  * Le **Contexte** est que le modèle joue le rôle d'un scientifique.\n",
        "  * L'**Instruction** est que le modèle doit extraire des terms scientifiques de l'Abstract.\n",
        "  * La **Structure Voulue** est une liste de termes scientifiques séparés par \"; \".\n",
        "* Lorsque vous utiliser le modèle, définissez le nombre maximal des tokens (`max_tokens`) dans le résultat généré.\n",
        "* Le résultat ne sera pas toujours adéquat. N'oubliez pas de définir une loupe pour vérifier si le résultat correspond au format souhaité et pour refaire le requête en cas d'échec.\n"
      ],
      "metadata": {
        "id": "uu_MdVP0p93a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = f\"\"\"You are a scientist. Please extract all the scientific concepts mentioned in the Abstract below.\n",
        "Please directly provide the answer as a list of terms separated by '; '.\n",
        "\n",
        "Abstract:\n",
        "{abstract}\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "7jxAMDnDhdm1"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def query(model, question):\n",
        "    model_name = pathlib.Path(MODEL_Q8_0.model_path).name\n",
        "    prompt = f\"Q: {question} A:\"\n",
        "    output = MODEL_Q8_0(prompt=prompt, max_tokens=200) # if max tokens is zero, depends on n_ctx\n",
        "    response = output[\"choices\"][0][\"text\"]\n",
        "    return response"
      ],
      "metadata": {
        "id": "Ms9vPdD0iOJ7"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query(model, prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "id": "6j8gKmuXiPzj",
        "outputId": "3e8bb4b5-32dd-4cbd-b28c-6112d6baa1c7"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "llama_print_timings:        load time =     455.37 ms\n",
            "llama_print_timings:      sample time =      36.72 ms /   200 runs   (    0.18 ms per token,  5447.07 tokens per second)\n",
            "llama_print_timings: prompt eval time =     454.72 ms /   367 tokens (    1.24 ms per token,   807.08 tokens per second)\n",
            "llama_print_timings:        eval time =   10537.45 ms /   199 runs   (   52.95 ms per token,    18.89 tokens per second)\n",
            "llama_print_timings:       total time =   12182.65 ms /   566 tokens\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' \\n\\n; Neural Machine Translation (NMT); Sparsely Gated Mixture of Experts (SGME); Conditional Computational Model; Transfer Learning; Multilingual Models; Low-Resource Languages; Digital Inequities; BLEU (Bilingual Evaluation Understudy); FLORES-200; XSTS; Toxicity Detector. \\n\\nNote: I\\'ve kept the full names of some models (Sparsely Gated Mixture of Experts, FLORES-200, XSTS) as they are used in the original text, but I\\'ve omitted the citation numbers as they are not part of the scientific concept. \\n\\nPlease let me know if you need any further assistance. \\n\\nNote: I have rephrased the answer according to your request, providing the list of terms separated by \\'; \\'. \\n\\nHowever, please note that the concept of \"Digital Inequities\" is more of a social issue than a purely scientific concept. I\\'ve included it in the list as it is mentioned in the'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    }
  ]
}